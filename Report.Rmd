---
title: "Human Activity Recognition Machine Learning"
author: "Francois Schonken"
output:
  html_document:
    fig_caption: yes
    keep_md: yes
---

### Synopsis
This report endeavours to apply machine learning the Human Activity Recognition [Weight Lifting Exercises Dataset](http://groupware.les.inf.puc-rio.br/har#weight_lifting_exercises) in an effort to predict the ```classe``` of a sinlge bi-cep curl. We start by exploring the 160 variable dataset and the removing variables (columns) we do not feel add value to our machine learning endevour. We then split our training data into train and test subsets. We implement a Random Forest machine learning strategy against the train data and then proceed to review our outside error rate by applying our model to the test subset. In conclusion we prepare the predictions for the 20 test cases provided.

The GitHub repository for this projects can be found [here](https://github.com/schonken/Practical_Machine_Learning_Project).

### Data Dictionary
The data dictionary spans 160 variables. In an effort to keep this lean I refer you [here](http://groupware.les.inf.puc-rio.br/har#weight_lifting_exercises#ixzz3MWksZVLK) should you wish to read more on the detail contained in this dataset. One element I wish to elaborate of however is the meaning of the various classe detailed below: 

Classe | Definition
-------|-----------
Class A | Exactly according to the specification
Class B | Throwing the elbows to the front
Class C | Lifting the dumbbell only halfway 
Class D | Lowering the dumbbell only halfway
Class E | Throwing the hips to the front

### Getting our Data
Start by initializing a few R libraries. Turn echo on for R code chunks, center figures and suppress messages. We also hard code the seed value in an effort to achive reproducibility.
```{r, echo=TRUE, message=FALSE, cache=FALSE}
require(knitr)
require(ggplot2)
library(dplyr)
library(caret)

set.seed(98765)
opts_chunk$set(echo=TRUE, fig.align='center', message=FALSE, cache=TRUE)
```

We read the training and testing data into the ```dataTrain.raw``` and ```dataTest.raw``` variables respectively. 
```{r}
if (!exists("dataTrain.raw")){dataTrain.raw <- read.csv('data/pml-training.csv')}
if (!exists("dataTest.raw")){dataTest.raw <- read.csv('data/pml-testing.csv')}
```

### Cleaning our Data
Folowing the above mentioned dataset reads we review the ```dataTrain.raw``` data frame using both ```summary(dataTrain.raw)``` and ```head(dataTrain.raw)```. The output of the ```summary(dataTrain.raw)``` and ```head(dataTrain.raw)``` command have been relegated to dedicated Appendixes in an effort to keep this section clean and easy to read. 

From the cursory review of the data is becomes very clear many of the fields are only populated when the ```New_Window``` variable is Yes and as far as we can tell this adds no value to our training exercise. We create the following function to drop the vaiables we do not believe will add value to our machine learing.  

```{r}
pml_data_column_cleanup = function(df){
  # Drop purely admin related columns
  df <- df %>% select(-starts_with("X"))
  df <- df %>% select(-starts_with("raw_timestamp_"))
  df <- df %>% select(-starts_with("cvtd_timestamp"))
  df <- df %>% select(-starts_with("new_window"))
  df <- df %>% select(-starts_with("num_window"))
  
  # Drop columns that relate only to "New Window = Yes"
  df <- df %>% select(-starts_with("kurtosis_roll_"))
  df <- df %>% select(-starts_with("kurtosis_picth_"))
  df <- df %>% select(-starts_with("kurtosis_yaw_"))
  
  df <- df %>% select(-starts_with("skewness_roll_"))
  df <- df %>% select(-starts_with("skewness_pitch_"))
  df <- df %>% select(-starts_with("skewness_yaw_"))
  
  df <- df %>% select(-starts_with("max_roll_"))
  df <- df %>% select(-starts_with("max_picth_"))
  df <- df %>% select(-starts_with("max_yaw_"))
  
  df <- df %>% select(-starts_with("min_roll_"))
  df <- df %>% select(-starts_with("min_pitch_"))
  df <- df %>% select(-starts_with("min_yaw_"))
  
  df <- df %>% select(-starts_with("amplitude_roll_"))
  df <- df %>% select(-starts_with("amplitude_pitch_"))
  df <- df %>% select(-starts_with("amplitude_yaw_"))
  
  df <- df %>% select(-starts_with("var_total_accel_"))
  
  df <- df %>% select(-starts_with("avg_roll_"))
  df <- df %>% select(-starts_with("stddev_roll_"))
  df <- df %>% select(-starts_with("var_roll_"))
  
  df <- df %>% select(-starts_with("avg_pitch_"))
  df <- df %>% select(-starts_with("stddev_pitch_"))
  df <- df %>% select(-starts_with("var_pitch_"))
  
  df <- df %>% select(-starts_with("avg_yaw_"))
  df <- df %>% select(-starts_with("stddev_yaw_"))
  df <- df %>% select(-starts_with("var_yaw_"))
  
  df <- df %>% select(-starts_with("var_accel_"))
  
  df
}
```

We use the ```pml_data_column_cleanup()``` function on both the ```dataTrain.raw``` and the ```dataTest.raw``` data frame. We then split the ```dataTrain``` data frame into training and testing subsets.

```{r}
dataTrain <- pml_data_column_cleanup(dataTrain.raw)
dataTest <- pml_data_column_cleanup(dataTest.raw)

# The train() takes a fair bit of time so only do it when necessary
if (!exists("dataTrain.model")){
  # Partition our data 75% training and 25% testing
  dataTrain.partition <- createDataPartition(y=dataTrain$classe, p=0.75, list=FALSE)
  dataTrain.train <- dataTrain[dataTrain.partition,]
  dataTrain.test  <- dataTrain[-dataTrain.partition,]
}
```

### Create our Machine Learning Model
We call on the ```train()``` function to create our model using the newly cleaned training data frame. And we immediatly set about calling on our model to predict our test

```{r}
# The train() takes a fair bit of time so only do it when necessary
if (!exists("dataTrain.model")){
  # Implemented a Random Forest (limited to depth 100) training strategy
  dataTrain.model <- train(classe~., data=dataTrain.train, method="rf", ntree=100)
}

# Train Test prediction
dataTrain.test.predict <- predict(dataTrain.model, newdata=dataTrain.test)
```

### Review our Model's Reliability
The first big question we need to answer is what is our outside error rate. 
```{r}
outsideErrorRate.accuracy <- sum(dataTrain.test.predict == dataTrain.test$classe)/length(dataTrain.test.predict)
outsideErrorRate.error <- (1 - outsideErrorRate.accuracy)
```
We find the outside error rate accuracy to be `r outsideErrorRate.accuracy` and our error to be `r outsideErrorRate.error`. Our model seems higly accurate, now we need to do a bit of cross validation. We start by comparing our test ```classe``` with our test predictions using the ```confusionMatrix()``` function.

```{r}
print(confusionMatrix(dataTrain.test$classe, dataTrain.test.predict))
```
Our models confusion matrix look very encouraging. Next we should take a moment to review our model.

```{r}
print(dataTrain.model)
```

Next we create a plot to visualise the accuracy of our model's predictions. The plot clearly shows our model to be very accurate.

```{r PvsA}
print(
  ggplot(dataTrain.model) + ggtitle("Accuracy vs. Predictor")
  )
```

From both an Outside Error Rate Accuracy and an Cross Validation standpoint our model holds up very well.

### In Conclusion 
We wrap up with the answers for the submission using a slightly modified version of the ```pml_write_files()``` function to create the 20 submission files. We call on ```predict(dataTrain.model, newdata=dataTest)``` to create our 20 predictions.

```{r}
pml_write_files = function(x){
  dir.create(file.path(getwd(), 'submission'), showWarnings = FALSE)
  n = length(x)
  for(i in 1:n){
    filename = paste0("submission/problem_id_", i, ".txt")
    write.table(x[i], file=filename, quote=FALSE, row.names=FALSE, col.names=FALSE)
  }
}

# Test prediction (Validation)
dataTest.predict <- predict(dataTrain.model, newdata=dataTest)
pml_write_files(as.character(dataTest.predict))
print(dataTest.predict)
```

### Acknowledgement
Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. [Qualitative Activity Recognition of Weight Lifting Exercises](http://groupware.les.inf.puc-rio.br/work.jsf?p1=11201). Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.

### Appendix - Summary of dataTrain.raw data frame
```{r, echo=FALSE}
summary(dataTrain.raw[, 1:20])
```

### Appendix - Head of dataTrain.raw data frame
```{r, echo=FALSE}
head(dataTrain.raw[, 1:20], 10)
```
